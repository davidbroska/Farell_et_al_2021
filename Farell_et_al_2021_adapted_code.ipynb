{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Farell et al 2021 original code",
      "provenance": [],
      "authorship_tag": "ABX9TyME9myKESVDgdxS7+NVbh2r",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/davidbroska/Farell_et_al_2021/blob/main/Farell_et_al_2021_adapted_code.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uu7A6hRFtDk2"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AvQZ8yWAjI8v",
        "outputId": "8d1e1460-2585-475b-a220-df3f20cbf0b7"
      },
      "source": [
        "\n",
        "# MonteCarlo_simulation_average_treatment_effects_using_NNs.py\n",
        "# -----------------------------------------------------------------\n",
        "# Date: November 2018\n",
        "# Last modified: July 2020\n",
        "# This is the Python code for a Monte Carlo simulation used to support\n",
        "# findings of Farrell, M.H., Liang, T. and Misra, S., 2019:\n",
        "# 'Deep neural networks for estimation and inference',\n",
        "# arXiv preprint arXiv:1809.09953.\n",
        "# ------------------------------------------------------------------\n",
        "\n",
        "# downgrade tensorflow to older version https://github.com/tensorflow/tensorflow/issues/34431#issuecomment-609396713\n",
        "%tensorflow_version 1.x\n",
        "\n",
        "\n",
        "import os\n",
        "# Stopping Tensorflow from printing info messages and warnings.\n",
        "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.special import comb\n",
        "from itertools import combinations_with_replacement\n",
        "import tensorflow as tf\n",
        "import logging\n",
        "import time\n",
        "import random\n",
        "import sys\n",
        "\n",
        "# Stopping deprecation warnings\n",
        "logging.getLogger('tensorflow').disabled = True\n",
        "\n",
        "'''\n",
        "FLAGS options\n",
        "-------------\n",
        "    FLAGS.update: bool\n",
        "        If True the simulation results will be saved in appropriate\n",
        "        .csv file.\n",
        "    FLAGS.plot_true: bool\n",
        "        Turn plotting on or off. If many simulations are run, set\n",
        "        FLAGS.plot_true = False since the plots are saved in memory\n",
        "        until the program finishes, which can exhaust memory.\n",
        "    FLAGS.verbose: bool\n",
        "        Turn printing detailed messages on or off. If you run more than\n",
        "        a few simulations, you probably want to turn it off.\n",
        "    FLAGS.nsimulations: int\n",
        "        Number of simulations to run.\n",
        "    FLAGS.nconsumer_characteristics: int\n",
        "        Number of consumer characteristics in the artificial dataset.\n",
        "        In our simulations, we used 20 and 100 consumer characteristics.\n",
        "        It shouldn't be less than 20.\n",
        "    FLAGS.treatment: {'random' , 'not_random'}\n",
        "        If 'random', consumers are being treated at random. Otherwise,\n",
        "        probability of being treated is a function of consumer\n",
        "        characteristics.\n",
        "    FLAGS.model: {'simple' , 'quadratic'}\n",
        "        If 'simple', coefficients a and b in the artificial dataset\n",
        "        depend linearly on consumer characteristics. Otherwise, the\n",
        "        dependence is quadratic.\n",
        "    FLAGS.architecture: {architecture_1_, architecture_2_, ... ,\n",
        "        architecture_9_}\n",
        "        Which NN architecture to use. See below for parameters of these\n",
        "        architectures. To experiment with other NN architectures either\n",
        "        edit one of the existing architecture parameters or add\n",
        "        'architecture_10_' and define parameters in a similar manner.\n",
        "        Make sure to change architecture flag parameter value below as\n",
        "        well. Options for activations functions are 'relu', 'lrelu',\n",
        "        'prelu','srelu', 'plu', 'elu', 'none'.\n",
        "    FLAGS.data_seed: int or None\n",
        "        Which seed number to use for creation of fake dataset. If it is\n",
        "        set to None, a random seed is used.\n",
        "\n",
        "Model parameters\n",
        "----------------\n",
        "    train_proportion: scalar\n",
        "        A proportion of the dataset to be used for training. Has to be\n",
        "        between 0 and 1. If it is set to 1, than early_stopping is set\n",
        "        to False and the NN will be trained on the whole dataset for the\n",
        "        max_nepochs. The parameters will be retrieved at the last epoch.\n",
        "        Otherwise, training will be stopped when there is no improvement\n",
        "        of the loss on the validation set for max_epochs_without_change\n",
        "        or when max_nepochs is reached. The parameters will be retrieved\n",
        "        at the epoch where best validation loss is recorded.\n",
        "    max_nepochs: int\n",
        "        Maximum number of epochs for which NNs will be trained.\n",
        "    max_epochs_without_change: int\n",
        "        Number of epochs with no improvement on the validation loss to\n",
        "        wait before stopping the training.\n",
        "    hidden_layer_sizes: list of ints\n",
        "        Hidden layers for the first NN that estimates the treatment\n",
        "        coefficients. Length of the list defines the number of hidden\n",
        "        layers. Entries of the list define the size of each hidden\n",
        "        layer.\n",
        "    activation_functions: list of {'relu', 'lrelu', 'prelu','srelu',\n",
        "        'plu', 'elu', 'none'}\n",
        "        Which activation function to use on each hidden layer of the\n",
        "        first NN. Has to be of length len(hidden_layer_sizes) + 1. The\n",
        "        last element in the list should be 'none', because it\n",
        "        corresponds to the output layer.\n",
        "    dropout_rates_train: list of floats\n",
        "        Dropout rate on input and each hidden layer of the first NN.\n",
        "        Has to be of length len(hidden_layer_sizes) + 1 .\n",
        "    hidden_layer_sizes_treatment: list of ints\n",
        "        Hidden layers for the second NN that estimates the propensity\n",
        "        scores.\n",
        "    activation_functions_treatment: list of {'relu', 'lrelu', 'prelu',\n",
        "        'srelu', 'plu', 'elu', 'none'}\n",
        "        Which activation function to use on each hidden layer of the\n",
        "        second NN.\n",
        "    dropout_rates_train_treatment: list of floats\n",
        "        Dropout rate on each hidden layer of the second NN.\n",
        "    optimizer: {'RMSProp' , 'GradientDescent' , 'Adam'}\n",
        "        Which optimizer to use. In all of the simulations reported in\n",
        "        the paper, the 'Adam' optimizer was used.\n",
        "    learning_rate: scalar\n",
        "        Learning rate.\n",
        "    batch_size: int or None\n",
        "        Batch size. If int, batch_size should be smaller than the\n",
        "        length of the training set. To train on the whole training set\n",
        "        rather than on mini-batches, set to None.\n",
        "    alpha: float\n",
        "        Regularization strength parameter.\n",
        "    r: float\n",
        "        Mixing ratio of Ridge and Lasso regression. If it's equal to 0.,\n",
        "        than the regularization is equal to Ridge regression. If it is\n",
        "        equal to 1., it is equal to Lasso regression.\n",
        "    nconsumers: int\n",
        "        Number of consumers.\n",
        "'''\n",
        "FLAGS = tf.app.flags.FLAGS\n",
        "\n",
        "tf.app.flags.DEFINE_boolean('update', False,\n",
        "                            \"\"\"Record the simulation results.\"\"\")\n",
        "tf.app.flags.DEFINE_boolean('plot_true', False, \"\"\"Show plots.\"\"\")\n",
        "tf.app.flags.DEFINE_boolean('verbose', True, \"\"\"Show detailed messages.\"\"\")\n",
        "tf.app.flags.DEFINE_integer('nsimulations', 1,\n",
        "                            \"\"\"How many simulations to run.\"\"\")\n",
        "tf.app.flags.DEFINE_integer('nconsumer_characteristics', 100,\n",
        "                            \"\"\"Number of consumer characteristics.\"\"\")\n",
        "tf.app.flags.DEFINE_string('treatment', 'not_random',\n",
        "                           \"\"\"Are customers treated at random or not.\"\"\")\n",
        "tf.app.flags.DEFINE_string('model', 'quadratic',\n",
        "                           \"\"\"Is the mapping from consumer characteristics\n",
        "                           to their preferences linear or quadratic.\"\"\")\n",
        "tf.app.flags.DEFINE_string('architecture', 'architecture_1_',\n",
        "                           \"\"\"Which NN architecture to use.\"\"\")\n",
        "tf.app.flags.DEFINE_integer('data_seed', None,\n",
        "                            \"\"\"Seed to use to create fake data.\"\"\")\n",
        "\n",
        "remaining_args = FLAGS([sys.argv[0]] +\n",
        "                       [flag for flag in sys.argv if flag.startswith(\"--\")])\n",
        "assert (remaining_args == [sys.argv[0]])\n",
        "\n",
        "# Different architectures for the first NN\n",
        "if FLAGS.architecture == 'architecture_1_':\n",
        "    hidden_layer_sizes = [20, 10, 5]\n",
        "    dropout_rates_train = [0, 0, 0, 0]\n",
        "    activation_functions = ['relu', 'relu', 'relu', 'none']\n",
        "\n",
        "elif FLAGS.architecture == 'architecture_2_':\n",
        "    hidden_layer_sizes = [60, 30, 20]\n",
        "    dropout_rates_train = [0, 0, 0, 0]\n",
        "    activation_functions = ['relu', 'relu', 'relu', 'none']\n",
        "\n",
        "elif FLAGS.architecture == 'architecture_3_':\n",
        "    hidden_layer_sizes = [80, 80, 80]\n",
        "    dropout_rates_train = [0, 0, 0, 0]\n",
        "    activation_functions = ['relu', 'relu', 'relu', 'none']\n",
        "\n",
        "elif FLAGS.architecture == 'architecture_4_':\n",
        "    hidden_layer_sizes = [20, 15, 10, 5]\n",
        "    activation_functions = ['relu', 'relu', 'relu', 'relu', 'none']\n",
        "    dropout_rates_train = [0, 0, 0, 0, 0]\n",
        "\n",
        "elif FLAGS.architecture == 'architecture_5_':\n",
        "    hidden_layer_sizes = [60, 30, 20, 10]\n",
        "    activation_functions = ['relu', 'relu', 'relu', 'relu', 'none']\n",
        "    dropout_rates_train = [0, 0, 0, 0, 0]\n",
        "\n",
        "elif FLAGS.architecture == 'architecture_6_':\n",
        "    hidden_layer_sizes = [80, 80, 80, 80]\n",
        "    activation_functions = ['relu', 'relu', 'relu', 'relu', 'none']\n",
        "    dropout_rates_train = [0, 0, 0, 0, 0]\n",
        "\n",
        "elif FLAGS.architecture == 'architecture_7_':\n",
        "    hidden_layer_sizes = [20, 15, 15, 10, 10, 5]\n",
        "    dropout_rates_train = [0, 0, 0, 0, 0, 0, 0]\n",
        "    activation_functions = [\n",
        "        'relu', 'relu', 'relu', 'relu', 'relu', 'relu', 'none'\n",
        "    ]\n",
        "\n",
        "elif FLAGS.architecture == 'architecture_8_':\n",
        "    hidden_layer_sizes = [60, 30, 20, 20, 10, 5]\n",
        "    dropout_rates_train = [0, 0, 0, 0, 0, 0, 0]\n",
        "    activation_functions = [\n",
        "        'relu', 'relu', 'relu', 'relu', 'relu', 'relu', 'none'\n",
        "    ]\n",
        "\n",
        "elif FLAGS.architecture == 'architecture_9_':\n",
        "    hidden_layer_sizes = [80, 80, 80, 80, 80, 80]\n",
        "    dropout_rates_train = [0, 0, 0, 0, 0, 0, 0]\n",
        "    activation_functions = [\n",
        "        'relu', 'relu', 'relu', 'relu', 'relu', 'relu', 'none'\n",
        "    ]\n",
        "else:\n",
        "    raise ValueError('Architecture not found! Check the spelling.')\n",
        "\n",
        "if FLAGS.nconsumer_characteristics < 20:\n",
        "    raise ValueError('Number of consumer characteristics ' +\n",
        "                     'should not be less than 20.')\n",
        "\n",
        "dropout_rates_test = [0 for i in dropout_rates_train]\n",
        "\n",
        "# Architecture for the second NN that estimates\n",
        "# propensity scores\n",
        "hidden_layer_sizes_treatment = [50, 30]\n",
        "activation_functions_treatment = ['relu', 'relu', 'none']\n",
        "dropout_rates_train_treatment = [0, 0, 0]\n",
        "dropout_rates_test_treatment = [0 for i in dropout_rates_train_treatment]\n",
        "\n",
        "# Setting parameters values for generating fake data\n",
        "nconsumers = 10000\n",
        "\n",
        "# Run parameters\n",
        "train_proportion = 0.9\n",
        "max_nepochs = 5000\n",
        "max_epochs_without_change = 30\n",
        "\n",
        "if train_proportion == 1:\n",
        "    early_stopping = False\n",
        "else:\n",
        "    early_stopping = True\n",
        "\n",
        "optimizer = 'Adam'\n",
        "learning_rate = 0.009\n",
        "batch_size = 128\n",
        "batch_size_t = None\n",
        "\n",
        "# Regularization parameters\n",
        "alpha = 0.\n",
        "r = 0.2\n",
        "\n",
        "# Checking for spelling errors\n",
        "if not (FLAGS.model == 'quadratic' or FLAGS.model == 'simple'):\n",
        "    raise ValueError('Check whether model type is spelled correctly!')\n",
        "if not (FLAGS.treatment == 'random' or FLAGS.treatment == 'not_random'):\n",
        "    raise ValueError('Check whether treatment type is spelled correctly!')\n",
        "    \n",
        "start_time = time.time()\n",
        "\n",
        "X_train = T_train = Y_train = X_valid = \\\n",
        "    T_valid = Y_valid = X = T_real = Y = None\n",
        "\n",
        "\n",
        "# ---------------------- Function definitions ------------------------\n",
        "def running_time():\n",
        "    '''\n",
        "    Print the time passed since start_time.\n",
        "    '''\n",
        "    end_time = time.time()\n",
        "    hours = int((end_time-start_time) / 3600)\n",
        "    minutes = int((end_time-start_time) % 3600 / 60)\n",
        "    seconds = int(end_time - start_time - (3600*hours + 60*minutes))\n",
        "    print('Running time is: {} hours, {} minutes and {} seconds'.format(\n",
        "        hours, minutes, seconds))\n",
        "\n",
        "\n",
        "class FakeData():\n",
        "    '''\n",
        "    Create an artificial dataset with desired properties for testing\n",
        "    the NN method.\n",
        "\n",
        "    Inputs:\n",
        "    -------\n",
        "        N: int\n",
        "            Number of consumers.\n",
        "        nconsumer_characteristics: int\n",
        "            Number of consumer characteristics.\n",
        "        data seed: int or None\n",
        "            Seed used to create fake dataset. If it is set to None\n",
        "            than random seed is used.\n",
        "    '''\n",
        "    def __init__(self, N=nconsumers,\n",
        "                 nconsumer_characteristics=FLAGS.nconsumer_characteristics,\n",
        "                 data_seed=FLAGS.data_seed):\n",
        "\n",
        "        self.N = N\n",
        "        self.nconsumer_characteristics = nconsumer_characteristics\n",
        "        self.seed = data_seed\n",
        "\n",
        "        # Creating fake data variables\n",
        "        self.Y = None\n",
        "        self.X = None\n",
        "        self.mu0 = None\n",
        "        self.tau = None\n",
        "        self.T = None\n",
        "        self.prob_of_T = None\n",
        "        self.tau_true_mean = None\n",
        "\n",
        "    def _sum_polynomial_X_times_weights(self, weights):\n",
        "        '''\n",
        "        Evaluate the non-linear part of a quadratic polynomial in\n",
        "        consumer characteristics, self.X, with prescribed weights, w.\n",
        "\n",
        "        Inputs:\n",
        "        -------\n",
        "            weights: ndarray, shape = (num_additional_poly_terms, )\n",
        "                Weights corresponding to quadratic terms.\n",
        "        Outputs:\n",
        "        -------\n",
        "            sum_x: ndarray, shape = (N, 1)\n",
        "                Non-linear part of the quadratic polynomial evaluated\n",
        "                for each consumer.\n",
        "        '''\n",
        "        my_polynomial_indices = combinations_with_replacement(\n",
        "            list(range(self.nconsumer_characteristics)), 2)\n",
        "        i = 0\n",
        "        sum_x = 0\n",
        "        for p in my_polynomial_indices:\n",
        "            sum_x = sum_x + weights[i]*np.multiply(self.X[:, p[0]],\n",
        "                                                   self.X[:, p[1]])\n",
        "            i += 1\n",
        "        sum_x = sum_x.reshape(-1, 1)\n",
        "        return sum_x\n",
        "\n",
        "    def _create_TE_coefs(self, model):\n",
        "        '''\n",
        "        Create treatment effect coefficients.\n",
        "\n",
        "        Inputs:\n",
        "        -------\n",
        "            model: {'simple', 'quadratic'}\n",
        "                If 'simple' coefficients a and b in the artificial\n",
        "                dataset depend linearly on consumer characteristics.\n",
        "                Otherwise, the dependence is quadratic.\n",
        "        Outputs:\n",
        "        -------\n",
        "            bias_tau: float\n",
        "                Constant term in equation for tau.\n",
        "            alpha_tau: ndarray, shape = [nconsumer_characteristics, 1]\n",
        "                Linear coefficients in equation for tau.\n",
        "            beta_tau: ndarray, shape = [count]\n",
        "                Quadratic coefficients in equation for tau.\n",
        "                Count is the number of the second degree terms in a\n",
        "                quadratic polynomial where the number of variables is\n",
        "                equal to the number of consumer characteristics.\n",
        "        '''\n",
        "        np.random.seed(63)\n",
        "\n",
        "        # Calculating tau\n",
        "        alpha_tau = np.random.uniform(low=0.1, high=0.22,\n",
        "                                      size=[self.nconsumer_characteristics, 1])\n",
        "        bias_tau = -0.05\n",
        "        self.tau = np.dot(self.X, alpha_tau) + bias_tau\n",
        "\n",
        "        if model == 'quadratic':\n",
        "            count = comb(self.nconsumer_characteristics, 2, True, True)\n",
        "            beta_tau = np.random.uniform(low=-0.05, high=0.06, size=count)\n",
        "            self.tau = self.tau + self._sum_polynomial_X_times_weights(\n",
        "                beta_tau)\n",
        "        else:\n",
        "            beta_tau = None\n",
        "\n",
        "        # Calculating mu0\n",
        "        alpha_mu0 = np.random.normal(loc=0.3, scale=0.7,\n",
        "                                     size=[1, self.nconsumer_characteristics])\n",
        "        bias_mu0 = 0.09\n",
        "        self.mu0 = np.dot(self.X, alpha_mu0.T) + bias_mu0\n",
        "\n",
        "        if model == 'quadratic':\n",
        "            beta_mu0 = np.random.normal(loc=0.01,\n",
        "                                        scale=0.3,\n",
        "                                        size=count)\n",
        "            self.mu0 = self.mu0 + self._sum_polynomial_X_times_weights(\n",
        "                beta_mu0)\n",
        "        return alpha_tau, bias_tau, beta_tau\n",
        "\n",
        "    def _calculate_true_tau_mean(self, alpha_tau, bias_tau, beta_tau, model):\n",
        "        '''\n",
        "        Calculate true average treatment effect.\n",
        "\n",
        "        Inputs:\n",
        "        -------\n",
        "            bias_tau: float\n",
        "                Constant term in equation for tau.\n",
        "            alpha_tau: ndarray, shape = [nconsumer_characteristics, 1]\n",
        "                Linear coefficients in equation for tau.\n",
        "            beta_tau: ndarray, shape = [count]\n",
        "                Quadratic coefficients in equation for tau.\n",
        "                Count is the number of the second degree terms in a\n",
        "                quadratic polynomial where the number of variables is\n",
        "                equal to the number of consumer characteristics.\n",
        "            model: {'simple', 'quadratic'}\n",
        "                If 'simple' coefficients a and b in the artificial\n",
        "                dataset depend linearly on consumer characteristics.\n",
        "                Otherwise, the dependence is quadratic.\n",
        "        '''\n",
        "        X = 0.5\n",
        "\n",
        "        self.tau_true_mean = np.sum(X * alpha_tau) + bias_tau\n",
        "\n",
        "        if model == 'quadratic':\n",
        "            X_poly = 0.25 * np.ones(len(beta_tau))\n",
        "            s = 0\n",
        "            for i in range(self.nconsumer_characteristics):\n",
        "                X_poly[s] = 1/3.\n",
        "                s = s + self.nconsumer_characteristics - i\n",
        "\n",
        "            self.tau_true_mean = self.tau_true_mean + np.sum(X_poly * beta_tau)\n",
        "\n",
        "    def _create_propensity_scores(self, treatment):\n",
        "        '''\n",
        "        Calculate propensity scores and create treatment variable for\n",
        "        our fake dataset.\n",
        "\n",
        "        Inputs:\n",
        "        -------\n",
        "            treatment: {'random', 'not_random'}\n",
        "                    If 'random' consumers are being treated at random.\n",
        "                    Otherwise, probability of being treated is a function\n",
        "                    of consumer characteristics.\n",
        "        '''\n",
        "        if treatment == 'random':\n",
        "            self.prob_of_T = 0.5\n",
        "            self.T = np.random.binomial(\n",
        "                size=self.N, n=1, p=self.prob_of_T).reshape(self.N, 1)\n",
        "        else:\n",
        "            bias_p = 0.09\n",
        "            np.random.seed(72)\n",
        "            alpha_p = np.random.uniform(low=-0.55, high=0.55, size=[20, 1])\n",
        "            # Probability of t only depends on the first 20 consumers\n",
        "            # features\n",
        "            p_of_t = np.dot(self.X[:, :20], alpha_p) + bias_p\n",
        "            p_of_t = p_of_t.reshape(-1)\n",
        "            self.prob_of_T = 1 / (1+np.exp(-p_of_t))\n",
        "            self.T = np.random.binomial(size=self.N, n=1,\n",
        "                                        p=self.prob_of_T).reshape(self.N, 1)\n",
        "\n",
        "    def create_fake_data(self, model=FLAGS.model, verbose=FLAGS.verbose,\n",
        "                         treatment=FLAGS.treatment):\n",
        "        '''\n",
        "        Create an artificial dataset.\n",
        "\n",
        "        Consumer characteristics, X, and normal errors are generated for\n",
        "        each consumer randomly. Then coefficients mu0 and tau are\n",
        "        created as functions of consumer characteristics, X.\n",
        "\n",
        "        If treatment == 'random' all the consumers are treated with an\n",
        "        equal likelihood of 0.5. Otherwise, the propensity scores that\n",
        "        depend on consumer characteristics are calculated.\n",
        "\n",
        "        Finally, a target variable is created.\n",
        "\n",
        "        Inputs:\n",
        "        -------\n",
        "            model: {'simple', 'quadratic'}\n",
        "                If 'simple' coefficients a and b in the artificial\n",
        "                dataset depend linearly on consumer characteristics.\n",
        "                Otherwise, the dependence is quadratic.\n",
        "            verbose: bool\n",
        "                If True print detailed messages.\n",
        "            treatment: {'random', 'not_random'}\n",
        "                If 'random' consumers are being treated at random.\n",
        "                Otherwise, probability of being treated is a function\n",
        "                of consumer characteristics.\n",
        "        Outputs:\n",
        "        -------\n",
        "            self.Y: ndarray, shape = (N, 1)\n",
        "                Target value.\n",
        "            self.X: ndarray, shape = (N, nconsumer_characteristics)\n",
        "                Consumer characteristics.\n",
        "            self.mu0: ndarray, shape = (N, 1)\n",
        "                Mu0 for each consumer.\n",
        "            self.tau: ndarray, shape = (N, 1)\n",
        "                Tau for each consumer.\n",
        "            self.T: ndarray, shape = (N, 1)\n",
        "                Treatment for each consumer.\n",
        "            self.seed: int\n",
        "                Random seed used to create fake dataset.\n",
        "            self.prob_of_T: ndarray, shape = (N,)  or 0.5\n",
        "                Propensity scores for each consumer if treatment\n",
        "                variable is set to 'not_random'.\n",
        "            self.tau_true_mean: float\n",
        "                True average treatment effect.\n",
        "        '''\n",
        "        if self.seed is None:\n",
        "            self.seed = random.randint(1, 100000)\n",
        "        if verbose:\n",
        "            print('Seed number is: ', self.seed)\n",
        "        np.random.seed(self.seed)\n",
        "\n",
        "        self.X = np.random.uniform(\n",
        "            low=0, high=1, size=[self.N, self.nconsumer_characteristics])\n",
        "        normal_errors = np.random.normal(size=[self.N, 1], loc=0.0, scale=1.0)\n",
        "        alpha_tau, bias_tau, beta_tau = self._create_TE_coefs(model)\n",
        "        self._calculate_true_tau_mean(alpha_tau, bias_tau, beta_tau, model)\n",
        "        self._create_propensity_scores(treatment)\n",
        "        self.Y = self.mu0 + self.tau*self.T + normal_errors\n",
        "        return (self.Y, self.X, self.mu0, self.tau, self.T, self.seed,\n",
        "                self.prob_of_T, self.tau_true_mean)\n",
        "\n",
        "\n",
        "def get_train_test_inds(t):\n",
        "    '''\n",
        "    Split the dataset into training and validation sets while\n",
        "    preserving the proportion of targeted customers in both datasets.\n",
        "\n",
        "    Inputs:\n",
        "    -------\n",
        "        t: array-like, shape=(N, 1)\n",
        "            Treatment array.\n",
        "    Outputs:\n",
        "    -------\n",
        "        train_inds: array of bools\n",
        "            Indices of the training set.\n",
        "        valid_inds: array of bools\n",
        "            Indices of the validation set.\n",
        "    '''\n",
        "    t_array = np.array(t)\n",
        "    train_inds = np.zeros(len(t_array), dtype=bool)\n",
        "    valid_inds = np.zeros(len(t_array), dtype=bool)\n",
        "    values = np.unique(t_array)\n",
        "    for value in values:\n",
        "        value_inds = np.nonzero(t_array == value)[0]\n",
        "        np.random.shuffle(value_inds)\n",
        "        n = int(train_proportion * len(value_inds))\n",
        "        train_inds[value_inds[:n]] = True\n",
        "        valid_inds[value_inds[n:]] = True\n",
        "    return train_inds, valid_inds\n",
        "\n",
        "\n",
        "def calculate_batch_size(batch_size, X_train):\n",
        "    '''\n",
        "    If batch_size is int than do nothing, else if batch_size is\n",
        "    equal to None, set batch size to be of a size equal to the\n",
        "    length of the training dataset.\n",
        "\n",
        "    Inputs:\n",
        "    -------\n",
        "        batch_size: int or None\n",
        "            Batch size.\n",
        "        X_train: ndarray\n",
        "            Array of consumer characteristics on which to\n",
        "            perform training.\n",
        "    Outputs:\n",
        "    -------\n",
        "        batch_size: int\n",
        "            Batch size.\n",
        "    '''\n",
        "    if batch_size is None:\n",
        "        batch_size = len(X_train)\n",
        "    return batch_size\n",
        "\n",
        "\n",
        "def plu_activation(input_value, alpha_=0.1, c=1):\n",
        "    '''\n",
        "    Apply PLU activation function on the input value.\n",
        "\n",
        "    Inputs:\n",
        "    -------\n",
        "        input_value: Tensor\n",
        "            An input tensor on which to apply PLU activation function.\n",
        "        alpha: float\n",
        "            First parameter of PLU function.\n",
        "        c: float\n",
        "            Second parameter of PLU function.\n",
        "\n",
        "    Outputs:\n",
        "    -------\n",
        "            Transformed input values after applying PLU activation\n",
        "            function.\n",
        "    '''\n",
        "\n",
        "    return tf.maximum(alpha_*(input_value+c) - c,\n",
        "                      tf.minimum(alpha_*(input_value-c) + c,\n",
        "                                 input_value))\n",
        "\n",
        "\n",
        "def srelu_activation(input_value, scope_name):\n",
        "    '''\n",
        "    Apply S-shaped Rectified Linear activation function on the\n",
        "    input value.\n",
        "\n",
        "    Inputs:\n",
        "    -------\n",
        "        input_value: Tensor\n",
        "            An input tensor on which to apply SReLU activation.\n",
        "        scope_name: string\n",
        "            Scope name.\n",
        "\n",
        "    Outputs:\n",
        "    -------\n",
        "            Transformed input values after applying SReLU activation\n",
        "            function.\n",
        "    '''\n",
        "    with tf.variable_scope(scope_name):\n",
        "        t_right = tf.get_variable('t_right', input_value.get_shape()[-1],\n",
        "                                  initializer=tf.constant_initializer(0.),\n",
        "                                  dtype=tf.float32)\n",
        "        a_right = tf.get_variable(\n",
        "            'a_right', input_value.get_shape()[-1],\n",
        "            initializer=tf.initializers.random_uniform(minval=0, maxval=1),\n",
        "            dtype=tf.float32)\n",
        "        t_left = tf.get_variable(\n",
        "            't_left', input_value.get_shape()[-1],\n",
        "            initializer=tf.initializers.random_uniform(minval=0, maxval=5),\n",
        "            dtype=tf.float32)\n",
        "        a_left = tf.get_variable('a_left', input_value.get_shape()[-1],\n",
        "                                 initializer=tf.constant_initializer(1.),\n",
        "                                 dtype=tf.float32)\n",
        "\n",
        "    t_right_actual = t_left + tf.abs(t_right)\n",
        "    y_left_and_center = t_left + tf.keras.activations.relu(\n",
        "        input_value - t_left, a_left, t_right_actual - t_left\n",
        "    )\n",
        "    y_right = tf.nn.relu(input_value-t_right_actual) * a_right\n",
        "    return y_left_and_center + y_right\n",
        "\n",
        "\n",
        "def plotting_loss_functions(loss1, loss2=[], add_title=''):\n",
        "    '''\n",
        "    Plot the loss functions.\n",
        "\n",
        "    Inputs:\n",
        "    -------\n",
        "        loss1: list of floats\n",
        "            First list of recorded losses through epochs.\n",
        "        loss2: list of floats\n",
        "            Second list of recorded losses through epochs.\n",
        "        add_title: string\n",
        "            Addition to the title of the graph.\n",
        "    '''\n",
        "    plt.figure(figsize=(12, 5))\n",
        "    plt.clf()\n",
        "    plt.plot(range(len(loss1)), loss1, 'r-', lw=3)\n",
        "    if early_stopping:\n",
        "        plt.plot(range(len(loss2)), loss2, 'b-', lw=3)\n",
        "        plt.legend(['loss on training set',\n",
        "                    'loss on validation set'])\n",
        "        plt.title('Loss on training and validation set' + add_title,\n",
        "                  fontsize=14)\n",
        "    else:\n",
        "        plt.legend(['loss on training set'])\n",
        "        plt.title('Loss on training set' + add_title, fontsize=14)\n",
        "    plt.xlabel('Epoch number', fontsize=14)\n",
        "    plt.ylabel('Loss', fontsize=14)\n",
        "\n",
        "\n",
        "class NeuralNetwork():\n",
        "    '''\n",
        "    Create a neural network with specified properties.\n",
        "\n",
        "    Inputs:\n",
        "    -------\n",
        "        hidden_layer_sizes: list of ints\n",
        "            Length of the list defines the number of hidden layers.\n",
        "            Entries of the list define the number of hidden units in\n",
        "            each hidden layer.\n",
        "        activation_functions: list of {'relu', 'lrelu', 'prelu',\n",
        "                                       'srelu', 'plu', 'elu', 'none'}\n",
        "            Activation function for each layer.\n",
        "            Has to be of length len(hidden_layer_sizes) + 1.\n",
        "        dropout_rates_train:  list of floats\n",
        "            Dropout rate to be used during training for each layer.\n",
        "            Has to be of length len(hidden_layer_sizes) + 1.\n",
        "        batch_size: int\n",
        "            Batch size.\n",
        "        size_of_the_output: int\n",
        "            Number of units in the output layer.\n",
        "        nconsumer_characteristics: int\n",
        "            Number of consumer characteristics.\n",
        "        alpha: float\n",
        "            Regularization strength parameter.\n",
        "        r_par: float\n",
        "            Mixing ratio of Ridge and Lasso regression.\n",
        "            Has to be between 0 and 1.\n",
        "        max_epochs_without_change: int\n",
        "            Number of epochs with no improvement on the validation loss\n",
        "            to wait before stopping the training.\n",
        "        max_nepochs: int\n",
        "            Maximum number of epochs for which NNs will be trained.\n",
        "        optimizer: string\n",
        "            Optimizer\n",
        "        learning_rate: scalar\n",
        "            Learning rate.\n",
        "    '''\n",
        "    def __init__(self, hidden_layer_sizes, activation_functions,\n",
        "                 dropout_rates_train, batch_size, size_of_the_output,\n",
        "                 nconsumer_characteristics=FLAGS.nconsumer_characteristics,\n",
        "                 alpha=alpha, r_par=r,\n",
        "                 max_epochs_without_change=max_epochs_without_change,\n",
        "                 max_nepochs=max_nepochs, optimizer=optimizer,\n",
        "                 learning_rate=learning_rate):\n",
        "\n",
        "        self.hidden_layer_sizes = hidden_layer_sizes\n",
        "        self.activation_functions = activation_functions\n",
        "        self.dropout_rates_train = dropout_rates_train\n",
        "        self.dropout_rates_test = [0 for i in dropout_rates_train]\n",
        "        self.batch_size = batch_size\n",
        "        self.size_of_the_output = size_of_the_output\n",
        "        self.nconsumer_characteristics = nconsumer_characteristics\n",
        "        self.alpha = alpha\n",
        "        self.r_par = r_par\n",
        "        self.max_epochs_without_change = max_epochs_without_change\n",
        "        self.max_nepochs = max_nepochs\n",
        "        self.optimizer = optimizer\n",
        "        self.learning_rate = learning_rate\n",
        "\n",
        "    def _fully_connected_layer_builder(self, input_data, hidden_layer_size,\n",
        "                                       total_num_features, scope_name,\n",
        "                                       activation, dropout_rate):\n",
        "        '''\n",
        "        Build a fully connected layer within the NN.\n",
        "\n",
        "        Inputs:\n",
        "        -------\n",
        "            input_data: Tensor\n",
        "                Output from the previous layer.\n",
        "            hidden_layer_size: int\n",
        "                Size of the current layer.\n",
        "            total_num_features: int\n",
        "                Number of units from the previous layer.\n",
        "            scope_name: string\n",
        "                Scope name.\n",
        "            activation: {'relu', 'lrelu', 'prelu', 'srelu',\n",
        "                         'plu', 'elu', 'none'}\n",
        "                Activation function.\n",
        "            dropout_rate: scalar\n",
        "                Dropout rate. Has to be between 0 and 1.\n",
        "\n",
        "        Outputs:\n",
        "        -------\n",
        "            hid_layer_activation: Tensor\n",
        "                The hidden layer output.\n",
        "        '''\n",
        "        # Dropout:\n",
        "        input_data = tf.contrib.layers.dropout(\n",
        "            inputs=input_data, keep_prob=1-dropout_rate)\n",
        "\n",
        "        # Creating weights and bias terms for our fully connected layer\n",
        "        with tf.variable_scope(scope_name):\n",
        "            weights = np.sqrt(2) * tf.get_variable(\n",
        "                \"weights\",\n",
        "                shape=[total_num_features, hidden_layer_size],\n",
        "                initializer=tf.contrib.layers.xavier_initializer())\n",
        "            b = tf.Variable(tf.zeros([hidden_layer_size]), name='biases')\n",
        "\n",
        "        # Defining the fully connected neural network layer\n",
        "        hid_layer_activation = tf.matmul(input_data, weights) + b\n",
        "\n",
        "        if activation == 'relu':\n",
        "            hid_layer_activation = tf.nn.relu(hid_layer_activation)\n",
        "        elif activation == 'lrelu':\n",
        "            hid_layer_activation = tf.nn.leaky_relu(hid_layer_activation,\n",
        "                                                    alpha=0.2,\n",
        "                                                    name='lrelu')\n",
        "        elif activation == 'prelu':\n",
        "            prelu_act = tf.keras.layers.PReLU()\n",
        "            hid_layer_activation = prelu_act(hid_layer_activation)\n",
        "        elif activation == 'srelu':\n",
        "            hid_layer_activation = srelu_activation(hid_layer_activation,\n",
        "                                                    scope_name)\n",
        "        elif activation == 'plu':\n",
        "            hid_layer_activation = plu_activation(hid_layer_activation)\n",
        "        elif activation == 'elu':\n",
        "            hid_layer_activation = tf.nn.elu(hid_layer_activation)\n",
        "        elif activation == 'none':\n",
        "            pass\n",
        "        else:\n",
        "            raise ValueError('Activation function not recognized! ' +\n",
        "                             'Check the spelling.')\n",
        "        return hid_layer_activation\n",
        "\n",
        "    def _building_the_network(self, layer_input, dropout_rates):\n",
        "        '''\n",
        "        Build the whole fully connected NN.\n",
        "\n",
        "        Inputs:\n",
        "        -------\n",
        "            layer_input: Tensor\n",
        "                Input layer.\n",
        "            dropout_rates: list of floats\n",
        "                Dropout rate for each layer. Each entry has to\n",
        "                be between 0 and 1. Has to be of length\n",
        "                len(hidden_layer_sizes) + 1.\n",
        "\n",
        "        Outputs:\n",
        "        -------\n",
        "            output_fc_layer: Tensor\n",
        "                Output layer.\n",
        "        '''\n",
        "        hidden_layer_sizes_expand = self.hidden_layer_sizes + [\n",
        "            self.size_of_the_output, self.nconsumer_characteristics]\n",
        "\n",
        "        for i in range(len(self.hidden_layer_sizes) + 1):\n",
        "            output_fc_layer = self._fully_connected_layer_builder(\n",
        "                input_data=layer_input,\n",
        "                hidden_layer_size=hidden_layer_sizes_expand[i],\n",
        "                total_num_features=hidden_layer_sizes_expand[i-1],\n",
        "                scope_name='l' + str(i+1),\n",
        "                activation=self.activation_functions[i],\n",
        "                dropout_rate=dropout_rates[i]\n",
        "            )\n",
        "            layer_input = output_fc_layer\n",
        "        return output_fc_layer\n",
        "\n",
        "    def _building_the_network_estimates_TE(self, input_data, t_var,\n",
        "                                           y_var, dropout_rates):\n",
        "        '''\n",
        "        Build the neural network that estimates treatment\n",
        "        coefficients.\n",
        "\n",
        "        Inputs:\n",
        "        -------\n",
        "            layer_input: Tensor\n",
        "                Input layer.\n",
        "            t_var: Tensor\n",
        "                Treatment\n",
        "            y_var: Tensor\n",
        "                Target variable\n",
        "            dropout_rates: list of floats\n",
        "                Dropout rate for each layer. Each entry has to\n",
        "                be between 0 and 1. Has to be of length\n",
        "                len(hidden_layer_sizes) + 1.\n",
        "\n",
        "        Outputs:\n",
        "        -------\n",
        "            output: Tensor\n",
        "                Treatment coefficients.\n",
        "            loss: scalar\n",
        "                Loss without regularization.\n",
        "        '''\n",
        "\n",
        "        output = self._building_the_network(input_data, dropout_rates)\n",
        "        tau = output[:, 0:1]\n",
        "        mu0 = output[:, 1:2]\n",
        "        Y_predicted = tf.multiply(t_var, tau) + mu0\n",
        "\n",
        "        # Mean squared error loss:\n",
        "        loss = tf.losses.mean_squared_error(\n",
        "            labels=y_var, predictions=Y_predicted)\n",
        "        return output, loss\n",
        "\n",
        "    def _building_the_network_estimates_PS(self, input_data,\n",
        "                                           t_var, dropout_rates):\n",
        "        '''\n",
        "        Build the neural network that estimates propensity\n",
        "        scores.\n",
        "\n",
        "        Inputs:\n",
        "        -------\n",
        "            layer_input: Tensor\n",
        "                Input layer.\n",
        "            t_var: Tensor\n",
        "                Treatment\n",
        "            dropout_rates: list of floats\n",
        "                Dropout rate for each layer. Each entry has to\n",
        "                be between 0 and 1. Has to be of length\n",
        "                len(hidden_layer_sizes) + 1.\n",
        "\n",
        "        Outputs:\n",
        "        -------\n",
        "            output: Tensor\n",
        "                Output of the NN.\n",
        "            loss: scalar\n",
        "                Loss without regularization.\n",
        "        '''\n",
        "        output = self._building_the_network(input_data, dropout_rates)\n",
        "\n",
        "        # Calculating cross entropy loss\n",
        "        loss = tf.reduce_mean(\n",
        "            tf.nn.sigmoid_cross_entropy_with_logits(\n",
        "                labels=tf.reshape(t_var, [-1, ]),\n",
        "                logits=tf.reshape(output, [-1, ])))\n",
        "        return output, loss\n",
        "\n",
        "    def _calc_the_loss_with_reg(self, loss_before_regularization):\n",
        "        '''\n",
        "        Calculate loss with regularization.\n",
        "\n",
        "        Inputs:\n",
        "        -------\n",
        "            loss_before_regularization: scalar\n",
        "                Loss without regularization.\n",
        "\n",
        "        Outputs:\n",
        "        -------\n",
        "            total_loss: float\n",
        "                Loss with regularization.\n",
        "        '''\n",
        "        l1_l2_regularizer = tf.contrib.layers.l1_l2_regularizer(\n",
        "            scale_l1=self.alpha*self.r_par, scale_l2=self.alpha*(1-self.r_par))\n",
        "        regularization_term = tf.contrib.layers.apply_regularization(\n",
        "            l1_l2_regularizer, tf.trainable_variables(scope=r'l\\d+/weights*'))\n",
        "\n",
        "        total_loss = loss_before_regularization + regularization_term\n",
        "        return total_loss\n",
        "\n",
        "    def _optimize_the_loss_function(self, loss_with_regularization):\n",
        "        '''\n",
        "        Update the weights after one training step.\n",
        "\n",
        "        Inputs:\n",
        "        -------\n",
        "            loss_with_regularization: scalar\n",
        "                Loss with regularization.\n",
        "\n",
        "        Outputs:\n",
        "        -------\n",
        "            train_step: Operation that updates the weights\n",
        "        '''\n",
        "        if self.optimizer == 'RMSProp':\n",
        "            train_step = tf.train.RMSPropOptimizer(\n",
        "                self.learning_rate).minimize(loss_with_regularization)\n",
        "        if self.optimizer == 'GradientDescent':\n",
        "            train_step = tf.train.GradientDescentOptimizer(\n",
        "                self.learning_rate).minimize(loss_with_regularization)\n",
        "        if self.optimizer == 'Adam':\n",
        "            train_step = tf.train.AdamOptimizer(self.learning_rate).minimize(\n",
        "                loss_with_regularization)\n",
        "        return train_step\n",
        "\n",
        "    def _create_minibatches(self, X, T, Y, rest, shuffle=False):\n",
        "        '''\n",
        "        Create mini-batches generator. Yields a mini-batch of batch_size\n",
        "        length of consumer characteristics, X, treatments, T, target\n",
        "        values, Y, and the indices of the remaining dataset.\n",
        "\n",
        "        Inputs:\n",
        "        -------\n",
        "            X: ndarray, shape=(len(X_train), nconsumer_characteristics)\n",
        "                Array of consumer characteristics.\n",
        "            T: ndarray, shape=(len(X_train), 1)\n",
        "                Treatment array.\n",
        "            Y: ndarray, shape=(len(X_train), 1)\n",
        "                Target value array.\n",
        "            rest: list of ints\n",
        "                Indices of the remaining array from the previous run.\n",
        "            shuffle: bool\n",
        "                If True, shuffle the array.\n",
        "        Outputs:\n",
        "        -------\n",
        "            X[excerpt]: ndarray, shape=(batch_size,\n",
        "                                        nconsumer_characteristics)\n",
        "                Mini batch of consumer characteristics.\n",
        "            T[excerpt]: ndarray, shape=(batch_size, 1)\n",
        "                Mini batch of treatment values.\n",
        "            Y[excerpt]: ndarray, shape=(batch_size, 1)\n",
        "                Mini batch of target values.\n",
        "            rest: list of ints\n",
        "                Indices of the remaining array after current run.\n",
        "        '''\n",
        "        if shuffle:\n",
        "            indices1 = np.arange(X.shape[0])\n",
        "            np.random.shuffle(indices1)\n",
        "            indices = np.array(rest + list(indices1))\n",
        "\n",
        "        for start_idx in range(0, len(indices) - self.batch_size + 1,\n",
        "                               self.batch_size):\n",
        "            if shuffle:\n",
        "                excerpt = indices[start_idx:start_idx+self.batch_size]\n",
        "            else:\n",
        "                excerpt = slice(start_idx, start_idx+self.batch_size)\n",
        "\n",
        "            rest = list(indices[start_idx+self.batch_size:])\n",
        "            yield X[excerpt], T[excerpt], Y[excerpt], rest\n",
        "\n",
        "    def _training_the_NN(self, estimating_TE):\n",
        "        '''\n",
        "        Train a NN for max_nepochs or until early stopping criterion\n",
        "        is met.\n",
        "\n",
        "        Inputs:\n",
        "        -------\n",
        "            estimating_TE: bool\n",
        "                Is neural network used for estimating treatment\n",
        "                coefficients.\n",
        "\n",
        "        Outputs:\n",
        "        -------\n",
        "            best_loss: float\n",
        "                Minimum value of loss achieved on the validation set if\n",
        "                train_proportion less than 1. Otherwise, loss achieved\n",
        "                on the whole dataset during the last epoch.\n",
        "            epoch_best: int\n",
        "                Epoch at which minimum loss on validation set was\n",
        "                achieved if train_proportion less than 1. Otherwise,\n",
        "                equal to max_nepochs for which the NN is trained.\n",
        "            output_best: ndarray\n",
        "                Output of the NN at the epoch_best.\n",
        "             total_nparameters: int\n",
        "                 Number of neural network parameters\n",
        "        '''\n",
        "\n",
        "        # Placeholders\n",
        "        x = tf.placeholder(\n",
        "            tf.float32, shape=[None, FLAGS.nconsumer_characteristics])\n",
        "        t = tf.placeholder(tf.float32, shape=[None, 1])\n",
        "        y = tf.placeholder(tf.float32, shape=[None, 1])\n",
        "        dropout_rates = tf.placeholder(tf.float32, shape=[None])\n",
        "\n",
        "        if estimating_TE:\n",
        "            output, loss = self._building_the_network_estimates_TE(\n",
        "                x, t, y, dropout_rates)\n",
        "        else:\n",
        "            output, loss = self._building_the_network_estimates_PS(\n",
        "                x, t, dropout_rates)\n",
        "\n",
        "        total_loss = self._calc_the_loss_with_reg(loss)\n",
        "\n",
        "        train_step = self._optimize_the_loss_function(total_loss)\n",
        "\n",
        "        sess = tf.InteractiveSession()\n",
        "        # Initializing all variables\n",
        "        sess.run(tf.global_variables_initializer())\n",
        "        epoch_without_change = 0\n",
        "        break_cond = False\n",
        "\n",
        "        loss_train_list = []\n",
        "        rest = []\n",
        "        if early_stopping:\n",
        "            loss_validation_list = []\n",
        "            validation_loss_min = 10e6\n",
        "            feed_dict_valid = {\n",
        "                x: X_valid,\n",
        "                t: T_valid,\n",
        "                y: Y_valid,\n",
        "                dropout_rates: dropout_rates_test\n",
        "            }\n",
        "        else:\n",
        "            loss_whole_list = []\n",
        "\n",
        "        print(\"X_valid shape: \", X_valid.shape)\n",
        "\n",
        "        feed_dict_total = {\n",
        "            x: X,\n",
        "            t: T_real,\n",
        "            y: Y,\n",
        "            dropout_rates: dropout_rates_test\n",
        "        }\n",
        "\n",
        "        for i in range(self.max_nepochs):\n",
        "            if early_stopping:\n",
        "                loss_valid = total_loss.eval(feed_dict=feed_dict_valid)\n",
        "                loss_validation_list.append(loss_valid)\n",
        "            else:\n",
        "                loss_whole = total_loss.eval(feed_dict=feed_dict_total)\n",
        "                loss_whole_list.append(loss_whole)\n",
        "\n",
        "            if early_stopping:\n",
        "                if validation_loss_min > loss_valid:\n",
        "                    validation_loss_min = loss_valid\n",
        "                    output_best = output.eval(feed_dict=feed_dict_total)\n",
        "                    epoch_best = i\n",
        "                    epoch_without_change = 0\n",
        "                else:\n",
        "                    epoch_without_change += 1\n",
        "\n",
        "            s = 0\n",
        "            for mini_batch in self._create_minibatches(\n",
        "                    X_train, T_train, Y_train, rest, shuffle=True):\n",
        "\n",
        "                x_batch, t_batch, y_batch, rest = mini_batch\n",
        "                feed_dict_train = {\n",
        "                    x: x_batch,\n",
        "                    t: t_batch,\n",
        "                    y: y_batch,\n",
        "                    dropout_rates: self.dropout_rates_train\n",
        "                }\n",
        "                loss_train = sess.run(total_loss, feed_dict=feed_dict_train)\n",
        "                if s == 0:\n",
        "                    loss_train_list.append(loss_train)\n",
        "\n",
        "                if epoch_without_change > max_epochs_without_change:\n",
        "                    break_cond = True\n",
        "                    break\n",
        "                sess.run(train_step, feed_dict=feed_dict_train)\n",
        "                s += 1\n",
        "\n",
        "            if FLAGS.verbose:\n",
        "                if i % 25 == 0:\n",
        "                    if early_stopping:\n",
        "                        print('%d epoch:' % i, 'loss on validation set:',\n",
        "                              loss_valid)\n",
        "                    else:\n",
        "                        print('%d epoch:' % i, 'loss on whole set:',\n",
        "                              loss_whole)\n",
        "\n",
        "            # Check the stopping condition\n",
        "            if break_cond:\n",
        "                if FLAGS.verbose:\n",
        "                    print('Training is finished! ', end='')\n",
        "                    print('Best validation loss achieved at %d epoch'\n",
        "                          % epoch_best)\n",
        "                break\n",
        "\n",
        "        if not early_stopping:\n",
        "            output_best = output.eval(feed_dict=feed_dict_total)\n",
        "            epoch_best = i + 1\n",
        "            best_loss = loss_whole\n",
        "            loss_list = loss_whole_list\n",
        "        else:\n",
        "            best_loss = validation_loss_min\n",
        "            loss_list = loss_validation_list\n",
        "\n",
        "        # Num of N parameters\n",
        "        total_nparameters = np.sum([\n",
        "            np.product([xi.value for xi in x.get_shape()])\n",
        "            for x in tf.trainable_variables()])\n",
        "\n",
        "        # Plotting loss functions\n",
        "        if FLAGS.plot_true:\n",
        "            if estimating_TE:\n",
        "                add_title = ' - first NN'\n",
        "            else:\n",
        "                add_title = ' - second NN'\n",
        "\n",
        "            # If train_proportion less than 1, than loss_list represents\n",
        "            # list of losses on validation set after each epoch. If\n",
        "            # train_proportion = 1, then it is list of losses on whole\n",
        "            # dataset after each epoch.\n",
        "            plotting_loss_functions(\n",
        "                loss_train_list, loss_list, add_title)\n",
        "\n",
        "        # Close tf.InteractiveSession\n",
        "        sess.close()\n",
        "\n",
        "        return best_loss, epoch_best, output_best, total_nparameters\n",
        "\n",
        "    def training_the_NN_estimates_TE(self):\n",
        "        '''\n",
        "        Train a NN that estimates treatment coefficients for\n",
        "        max_nepochs or until early stopping criterion is met.\n",
        "\n",
        "        Outputs are the same as in ._training_the_NN function when\n",
        "        estimating_TE argument is set to True.\n",
        "        '''\n",
        "        return self._training_the_NN(estimating_TE=True)\n",
        "\n",
        "    def training_the_NN_estimates_PS(self):\n",
        "        '''\n",
        "        Train a NN that estimates propensity socres for\n",
        "        max_nepochs or until early stopping criterion is met.\n",
        "\n",
        "        Outputs are the same as in _training_the_NN function when\n",
        "        estimating_TE argument is set to False.\n",
        "        '''\n",
        "        return self._training_the_NN(estimating_TE=False)\n",
        "\n",
        "\n",
        "def influence_functions(mu0_pred, tau_pred, Y, T, prob_t_pred):\n",
        "    '''\n",
        "    Calculate the target value for each individual when treatment is\n",
        "    0 or 1.\n",
        "\n",
        "    Inputs:\n",
        "    -------\n",
        "        mu0_pred: ndarray, shape=(N, 1)\n",
        "        tau_pred: ndarray, shape=(N, 1)\n",
        "            Estimated conditional average treatment effect.\n",
        "        Y: ndarray, shape=(N,)\n",
        "            Target value array.\n",
        "        T: ndarray, shape=(N,)\n",
        "            Treatment array.\n",
        "        prob_t_pred: ndarray, shape=(N,)\n",
        "            Estimated propensity scores.\n",
        "    Outputs:\n",
        "    -------\n",
        "        psi_0: ndarray, shape=(N, 1)\n",
        "            Influence function for given x in case of no treatment.\n",
        "        psi_1: ndarray, shape=(N, 1)\n",
        "            Influence function for given x in case of treatment.\n",
        "    '''\n",
        "    first_part = (1-T) * (Y-mu0_pred)\n",
        "    second_part = T * (Y-mu0_pred-tau_pred)\n",
        "\n",
        "    if FLAGS.treatment == 'not_random':\n",
        "        prob_t_pred[prob_t_pred < 0.0001] = 0.0001\n",
        "        prob_t_pred[prob_t_pred > 0.9999] = 0.9999\n",
        "        psi_0 = (first_part/(1-prob_t_pred)) + mu0_pred\n",
        "        psi_1 = (second_part/prob_t_pred) + mu0_pred + tau_pred\n",
        "    else:\n",
        "        psi_0 = (first_part/(1-np.mean(T))) + mu0_pred\n",
        "        psi_1 = (second_part/np.mean(T)) + mu0_pred + tau_pred\n",
        "    return psi_0, psi_1\n",
        "\n",
        "\n",
        "def update_model_comparison_file(name, model_info, cols):\n",
        "    '''\n",
        "    Update .csv file with new model results.\n",
        "\n",
        "    Inputs:\n",
        "    -------\n",
        "        name: string\n",
        "            File name. If the file does not already exist creates a\n",
        "            new file. Otherwise appends new model results to the\n",
        "            existing file.\n",
        "        model_info: list\n",
        "            Results of the current run.\n",
        "        cols: list\n",
        "            Names of columns within the .csv file.\n",
        "            Has to be of the same length as model_info.\n",
        "    '''\n",
        "    if not os.path.isfile(name):\n",
        "        df = pd.DataFrame(columns=cols)\n",
        "        df.to_csv(name, index=False)\n",
        "        print('File does not exist. Creating new file!')\n",
        "    else:\n",
        "        print('File already exists. Appending model run!')\n",
        "\n",
        "    Model_comparison_Catalog_dataset = pd.read_csv(name)\n",
        "    ind = len(Model_comparison_Catalog_dataset['Model number'])\n",
        "    model_info[0][0] = ind\n",
        "    df = pd.DataFrame(model_info, columns=cols)\n",
        "    Model_comparison_Catalog_dataset = \\\n",
        "        Model_comparison_Catalog_dataset.append(df, ignore_index=True)\n",
        "    Model_comparison_Catalog_dataset.to_csv(name, index=False)\n",
        "\n",
        "\n",
        "def main():\n",
        "    print('-------------------------------------------------------')\n",
        "    print('Running Monte Carlo simulations for the following case:')\n",
        "    print('* %s treatment' % FLAGS.treatment)\n",
        "    print('* %s model' % FLAGS.model)\n",
        "    print('* %s consumer characteristics'\n",
        "          % FLAGS.nconsumer_characteristics, '\\n')\n",
        "    print('Using the following NN architectures:')\n",
        "    print('First NN hidden layer sizes: ', hidden_layer_sizes)\n",
        "    print('First NN hidden activations: ', activation_functions)\n",
        "    print('First NN dropout rates: ', dropout_rates_train, '\\n')\n",
        "\n",
        "    print('Second NN hidden layer sizes: ', hidden_layer_sizes_treatment)\n",
        "    print('Second NN hidden activations: ', activation_functions_treatment)\n",
        "    print('Second NN dropout rates: ', dropout_rates_train_treatment)\n",
        "    print('-------------------------------------------------------\\n')\n",
        "\n",
        "    count_in_interval = 0\n",
        "    for _ in range(FLAGS.nsimulations):\n",
        "        global X_train, T_train, Y_train, X_valid, \\\n",
        "            T_valid, Y_valid, X, T_real, Y\n",
        "        # ---------------- Creating the fake dataset -----------------\n",
        "\n",
        "        Y, X, mu0_real, tau_real, T_real, seed, prob_of_T,\\\n",
        "            tau_true_mean = FakeData().create_fake_data()\n",
        "\n",
        "        # Setting the seed to prevent randomness:\n",
        "        tf.set_random_seed(77)\n",
        "        np.random.seed(61)\n",
        "\n",
        "        # Splitting the dataset into training and validation set\n",
        "        if early_stopping:\n",
        "            train_inds, valid_inds = get_train_test_inds(T_real)\n",
        "            T_train = T_real[train_inds]\n",
        "            Y_train = Y[train_inds]\n",
        "            X_train = X[train_inds]\n",
        "            T_valid = T_real[valid_inds]\n",
        "            Y_valid = Y[valid_inds]\n",
        "            X_valid = X[valid_inds]\n",
        "        else:\n",
        "            T_train = T_real\n",
        "            Y_train = Y\n",
        "            X_train = X\n",
        "\n",
        "        #print(\"X_train\", X_train)\n",
        "        print(\"type X_train:\", type(X_train))\n",
        "        print(\"shape X_train:\", X_train.shape)\n",
        "        \n",
        "        # print(\"T_train\", T_train)\n",
        "        print(\"type T_train:\", type(T_train))\n",
        "        print(\"shape T_train:\", T_train.shape)\n",
        "        \n",
        "        #print(\"Y_train\", Y_train)\n",
        "        print(\"type Y_train\", type(Y_train))\n",
        "        print(\"shape Y_train\", Y_train.shape)\n",
        "        \n",
        "\n",
        "        # Determining batch size\n",
        "        batch_size_ = calculate_batch_size(batch_size, X_train)\n",
        "        batch_size_t_ = calculate_batch_size(batch_size_t, X_train)\n",
        "\n",
        "        # ------------- Building and training the first NN -----------\n",
        "\n",
        "        if FLAGS.verbose:\n",
        "            print('\\nTraining of treatment coefficients neural network:')\n",
        "        first_NN = NeuralNetwork(\n",
        "            hidden_layer_sizes, activation_functions, dropout_rates_train,\n",
        "            batch_size_, 2)\n",
        "\n",
        "        MSE_best, epoch_best, betas_pred_best, total_nparameters = \\\n",
        "            first_NN.training_the_NN_estimates_TE()\n",
        "\n",
        "        # ------------- Building and training the second NN ----------\n",
        "\n",
        "        if FLAGS.treatment == 'not_random':\n",
        "            # Reseting the graph\n",
        "            tf.reset_default_graph()\n",
        "\n",
        "            # Setting the seed to prevent randomness in our model:\n",
        "            tf.set_random_seed(77)\n",
        "            np.random.seed(61)\n",
        "\n",
        "            if FLAGS.verbose:\n",
        "                print('\\nTraining of propensity score neural network:')\n",
        "            second_NN = NeuralNetwork(\n",
        "                hidden_layer_sizes_treatment, activation_functions_treatment,\n",
        "                dropout_rates_train_treatment, batch_size_t_, 1)\n",
        "\n",
        "            CE_best, epoch_best_t, treat_best, total_nparameters_t = \\\n",
        "                second_NN.training_the_NN_estimates_PS()\n",
        "\n",
        "        # -------------------- Looking at the results ----------------\n",
        "\n",
        "        betas_pred = betas_pred_best\n",
        "        tau_pred = betas_pred[:, 0:1]\n",
        "        mu0_pred = betas_pred[:, 1:]\n",
        "\n",
        "        if FLAGS.treatment == 'not_random':\n",
        "            prob_of_t_pred = 1 / (1 + np.exp(-treat_best))\n",
        "\n",
        "        # Coefficients statistic\n",
        "        mu0_mean_pred = np.mean(mu0_pred)\n",
        "        std_mu0_pred = np.std(mu0_pred)\n",
        "        tau_mean_pred = np.mean(tau_pred)\n",
        "        std_tau_pred = np.std(tau_pred)\n",
        "\n",
        "        mu0_mean_real = np.mean(mu0_real)\n",
        "        std_mu0_real = np.std(mu0_real)\n",
        "        tau_mean_real = np.mean(tau_real)\n",
        "        std_tau_real = np.std(tau_real)\n",
        "\n",
        "        if FLAGS.verbose:\n",
        "            print('\\n------------------ mu0 results ------------------')\n",
        "            print(['Mean mu0_pred = %0.3f' % mu0_mean_pred,\n",
        "                   'Std mu0_pred = %0.3f' % std_mu0_pred])\n",
        "            print(['Mean mu0_real = %0.3f' % mu0_mean_real,\n",
        "                   'Std mu0_real = %0.3f' % std_mu0_real], '\\n')\n",
        "\n",
        "            print('------------------ tau results ------------------')\n",
        "            print(['Mean tau_pred = %0.3f' % tau_mean_pred,\n",
        "                   'Std tau_pred = %0.3f' % std_tau_pred])\n",
        "            print(['Mean tau_real = %0.3f' % tau_mean_real,\n",
        "                   'Std tau_real = %0.3f' % std_tau_real], '\\n')\n",
        "\n",
        "            if FLAGS.treatment == 'not_random':\n",
        "                print('------------------ t results ------------------')\n",
        "                print('Mean prob_of_t_pred = %0.3f' % np.mean(prob_of_t_pred),\n",
        "                      '\\nMean prob_of_t_real = %0.3f\\n' % np.mean(prob_of_T))\n",
        "\n",
        "        total_nparameters_t = np.sum([\n",
        "            np.product([xi.value for xi in x.get_shape()])\n",
        "            for x in tf.trainable_variables()\n",
        "        ])\n",
        "\n",
        "        if FLAGS.treatment == 'not_random':\n",
        "            psi_0, psi_1 = influence_functions(mu0_pred, tau_pred, Y,\n",
        "                                               T_real, prob_of_t_pred)\n",
        "        else:\n",
        "            psi_0, psi_1 = influence_functions(mu0_pred, tau_pred, Y,\n",
        "                                               T_real, prob_t_pred=None)\n",
        "\n",
        "        # Calculating confidence interval for average treatment effect\n",
        "        mean_diff_psi1_psi0 = np.mean(psi_1 - psi_0)\n",
        "        std_diff_psi1_psi0 = np.std(psi_1 - psi_0)\n",
        "        CI_upper_bound = (\n",
        "            mean_diff_psi1_psi0 + 1.96*std_diff_psi1_psi0/np.sqrt(nconsumers)\n",
        "        )\n",
        "        CI_lower_bound = (\n",
        "            mean_diff_psi1_psi0 - 1.96*std_diff_psi1_psi0/np.sqrt(nconsumers)\n",
        "        )\n",
        "\n",
        "        in_95_conf_int = CI_lower_bound < tau_true_mean < CI_upper_bound\n",
        "\n",
        "        print('is tau_true_mean in interval:', in_95_conf_int)\n",
        "        print('CI lower and upper bound are: (%0.3f, %0.3f)'\n",
        "              % (CI_lower_bound, CI_upper_bound))\n",
        "        if in_95_conf_int:\n",
        "            count_in_interval += 1\n",
        "\n",
        "        Y_pred = mu0_pred + tau_pred*T_real\n",
        "\n",
        "        # ----------------- Saving the results! ----------------------\n",
        "        name = 'Results_{}_model_{}{}_{}_{}.csv'.format(\n",
        "                FLAGS.model, FLAGS.architecture, FLAGS.treatment,\n",
        "                nconsumers, FLAGS.nconsumer_characteristics)\n",
        "\n",
        "        if FLAGS.treatment == 'random':\n",
        "            parameters_dict = {\n",
        "                'nconsumer_characteristics': FLAGS.nconsumer_characteristics,\n",
        "                'treatment': FLAGS.treatment, 'model': FLAGS.model,\n",
        "                'architecture': FLAGS.architecture,\n",
        "                'hidden_layer_sizes': hidden_layer_sizes,\n",
        "                'dropout_rates_train': dropout_rates_train,\n",
        "                'activation_functions': activation_functions,\n",
        "                'nconsumers': nconsumers,\n",
        "                'train_proportion': train_proportion,\n",
        "                'max_nepochs': max_nepochs,\n",
        "                'max_epochs_without_change': max_epochs_without_change,\n",
        "                'early_stopping': early_stopping, 'optimizer': optimizer,\n",
        "                'learning_rate': learning_rate, 'batch_size': batch_size,\n",
        "                'alpha': alpha, 'r': r}\n",
        "\n",
        "            cols = [\n",
        "                'Model number', 'seed', 'best_epoch', 'total_nparameters',\n",
        "                'Loss best', 'Mean mu0_pred', 'Std mu0_pred', 'Mean mu0_real',\n",
        "                'Std mu0_real', 'Mean tau_pred', 'Std tau_pred',\n",
        "                'Mean tau_real', 'Std tau_real', 'tau_true_mean',\n",
        "                'Y_real_mean', 'Y_pred_mean', 'CI_lower_bound',\n",
        "                'CI_upper_bound', 'psi_0_mean', 'psi_1_mean',\n",
        "                'mean_diff_psi_1_psi_0', 'std_diff_psi_1_psi_0', 'in_interval',\n",
        "                'model_parameters_dict']\n",
        "\n",
        "            model_info = [[\n",
        "                0, seed, epoch_best, total_nparameters,\n",
        "                MSE_best, mu0_mean_pred, std_mu0_pred, np.mean(mu0_real),\n",
        "                std_mu0_real, tau_mean_pred, std_tau_pred, tau_mean_real,\n",
        "                std_tau_real, tau_true_mean, np.mean(Y), np.mean(Y_pred),\n",
        "                CI_lower_bound, CI_upper_bound, np.mean(psi_0), np.mean(psi_1),\n",
        "                mean_diff_psi1_psi0, std_diff_psi1_psi0, in_95_conf_int,\n",
        "                parameters_dict]]\n",
        "\n",
        "        else:\n",
        "            parameters_dict = {\n",
        "                'nconsumer_characteristics': FLAGS.nconsumer_characteristics,\n",
        "                'treatment': FLAGS.treatment, 'model': FLAGS.model,\n",
        "                'architecture': FLAGS.architecture,\n",
        "                'hidden_layer_sizes': hidden_layer_sizes,\n",
        "                'dropout_rates_train': dropout_rates_train,\n",
        "                'activation_functions': activation_functions,\n",
        "                'hidden_layer_sizes_treatment': hidden_layer_sizes_treatment,\n",
        "                'activation_functions_treatment': activation_functions_treatment,\n",
        "                'dropout_rates_train_treatment': dropout_rates_train_treatment,\n",
        "                'nconsumers': nconsumers, 'train_proportion': train_proportion,\n",
        "                'max_nepochs': max_nepochs,\n",
        "                'max_epochs_without_change': max_epochs_without_change,\n",
        "                'early_stopping': early_stopping, 'optimizer': optimizer,\n",
        "                'learning_rate': learning_rate, 'batch_size': batch_size,\n",
        "                'batch_size_t': batch_size_t, 'alpha': alpha, 'r': r}\n",
        "\n",
        "            cols = [\n",
        "                'Model number', 'seed', 'best_epoch', 'best_epoch_t',\n",
        "                'total_nparameters', 'total_nparameters_t', 'Loss best',\n",
        "                'Loss best_treatment', 'Mean mu0_pred', 'Std mu0_pred',\n",
        "                'Mean mu0_real', 'Std mu0_real', 'Mean tau_pred',\n",
        "                'Std tau_pred', 'Mean tau_real', 'Std tau_real',\n",
        "                'tau_true_mean', 'Mean_prob_t_pred', 'Mean prob_of_t_real',\n",
        "                'mean_T_real', 'Y_real_mean', 'Y_pred_mean', 'CI_lower_bound',\n",
        "                'CI_upper_bound', 'psi_0_mean', 'psi_1_mean',\n",
        "                'mean_diff_psi_1_psi_0', 'std_diff_psi_1_psi_0', 'in_interval',\n",
        "                'model_parameters_dict']\n",
        "\n",
        "            model_info = [[\n",
        "                0, seed, epoch_best, epoch_best_t, total_nparameters,\n",
        "                total_nparameters_t, MSE_best, CE_best,\n",
        "                mu0_mean_pred, std_mu0_pred, mu0_mean_real, std_mu0_real,\n",
        "                tau_mean_pred, std_tau_pred, tau_mean_real, std_tau_real,\n",
        "                tau_true_mean, np.mean(prob_of_t_pred), np.mean(prob_of_T),\n",
        "                np.mean(T_real), np.mean(Y), np.mean(Y_pred), CI_lower_bound,\n",
        "                CI_upper_bound, np.mean(psi_0), np.mean(psi_1),\n",
        "                mean_diff_psi1_psi0, std_diff_psi1_psi0, in_95_conf_int,\n",
        "                parameters_dict]]\n",
        "\n",
        "        if FLAGS.update:\n",
        "            update_model_comparison_file(name, model_info, cols)\n",
        "        tf.reset_default_graph()  # restarting a NN graph\n",
        "        print('\\n')\n",
        "\n",
        "    print('%d out of %d simulations contain tau_true_mean in the CI'\n",
        "          % (count_in_interval, FLAGS.nsimulations))\n",
        "\n",
        "    # Print running time\n",
        "    running_time()\n",
        "\n",
        "    if os.path.exists(name):\n",
        "        model_data = pd.read_csv(name)\n",
        "        print(model_data)\n",
        "        print('%d out of %d simulations contain tau_true_mean in the CI'\n",
        "              % (np.sum(model_data['in_interval']), len(model_data)))\n",
        "        print('Name of the file:', name)\n",
        "    else:\n",
        "        print('File `' + name + '` is not yet created.')\n",
        "\n",
        "    # Plot all the graphs\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TensorFlow 1.x selected.\n",
            "-------------------------------------------------------\n",
            "Running Monte Carlo simulations for the following case:\n",
            "* not_random treatment\n",
            "* quadratic model\n",
            "* 100 consumer characteristics \n",
            "\n",
            "Using the following NN architectures:\n",
            "First NN hidden layer sizes:  [20, 10, 5]\n",
            "First NN hidden activations:  ['relu', 'relu', 'relu', 'none']\n",
            "First NN dropout rates:  [0, 0, 0, 0] \n",
            "\n",
            "Second NN hidden layer sizes:  [50, 30]\n",
            "Second NN hidden activations:  ['relu', 'relu', 'none']\n",
            "Second NN dropout rates:  [0, 0, 0]\n",
            "-------------------------------------------------------\n",
            "\n",
            "Seed number is:  79674\n",
            "type X_train: <class 'numpy.ndarray'>\n",
            "shape X_train: (8999, 100)\n",
            "type T_train: <class 'numpy.ndarray'>\n",
            "shape T_train: (8999, 1)\n",
            "type Y_train <class 'numpy.ndarray'>\n",
            "shape Y_train (8999, 1)\n",
            "\n",
            "Training of treatment coefficients neural network:\n",
            "X_valid shape:  (1001, 100)\n",
            "0 epoch: loss on validation set: 1535.7191\n",
            "25 epoch: loss on validation set: 4.620706\n",
            "50 epoch: loss on validation set: 4.5662127\n",
            "75 epoch: loss on validation set: 4.759458\n",
            "Training is finished! Best validation loss achieved at 58 epoch\n",
            "\n",
            "Training of propensity score neural network:\n",
            "X_valid shape:  (1001, 100)\n",
            "0 epoch: loss on validation set: 0.90569776\n",
            "25 epoch: loss on validation set: 0.6948213\n",
            "50 epoch: loss on validation set: 0.6936482\n",
            "75 epoch: loss on validation set: 0.6927007\n",
            "100 epoch: loss on validation set: 0.69099563\n",
            "125 epoch: loss on validation set: 0.688023\n",
            "150 epoch: loss on validation set: 0.6842963\n",
            "175 epoch: loss on validation set: 0.68280303\n",
            "200 epoch: loss on validation set: 0.6841231\n",
            "Training is finished! Best validation loss achieved at 177 epoch\n",
            "\n",
            "------------------ mu0 results ------------------\n",
            "['Mean mu0_pred = 31.896', 'Std mu0_pred = 4.894']\n",
            "['Mean mu0_real = 31.897', 'Std mu0_real = 5.234'] \n",
            "\n",
            "------------------ tau results ------------------\n",
            "['Mean tau_pred = 14.985', 'Std tau_pred = 1.128']\n",
            "['Mean tau_real = 14.794', 'Std tau_real = 1.348'] \n",
            "\n",
            "------------------ t results ------------------\n",
            "Mean prob_of_t_pred = 0.500 \n",
            "Mean prob_of_t_real = 0.500\n",
            "\n",
            "is tau_true_mean in interval: True\n",
            "CI lower and upper bound are: (14.705, 14.877)\n",
            "\n",
            "\n",
            "1 out of 1 simulations contain tau_true_mean in the CI\n",
            "Running time is: 0 hours, 0 minutes and 38 seconds\n",
            "File `Results_quadratic_model_architecture_1_not_random_10000_100.csv` is not yet created.\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}